import numpy as np
import pandas as pd
!unzip '/content/drive/MyDrive/BP_data_Cabrini2017_New.zip'
import numpy as np
import pandas as pd
import scipy.io as sio
import scipy.signal

def butter_lowpass_filter(signal, cutoff=5, fs=1000, order=3):
    nyquist = 0.4 * fs
    normal_cutoff = cutoff / nyquist
    b, a = scipy.signal.butter(order, normal_cutoff, btype='low', analog=False)
    return scipy.signal.filtfilt(b, a, signal)

def process_mat_files(file_paths, fs=1000, cutoff=5, order=3, output_csv=None):
    all_rows = []
    row_info = []

    for file_path in file_paths:
        print(f"Processing file: {file_path}")
        try:
            mat_contents = sio.loadmat(file_path)
            data = mat_contents['data']
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
            continue

        for i in range(180):
            try:
                ecg_signal    = data[0][i][0][0][0][2]
                ppg_signal    = data[0][i][0][0][0][4]
                bioimp_signal = data[0][i][0][0][0][3]
            except Exception as e:
                print(f"Index error in file {file_path} at index {i}: {e}")
                continue

            ecg_filtered    = butter_lowpass_filter(ecg_signal, cutoff=cutoff, fs=fs, order=order)
            ppg_filtered    = butter_lowpass_filter(ppg_signal, cutoff=cutoff, fs=fs, order=order)
            bioimp_filtered = butter_lowpass_filter(bioimp_signal, cutoff=cutoff, fs=fs, order=order)

            combined_row = np.concatenate([ecg_filtered, ppg_filtered, bioimp_filtered])
            all_rows.append(combined_row)
            row_info.append((file_path, i))

    if len(all_rows) == 0:
        raise ValueError("No data was processed. Please check the file paths and data structure.")

    total_signal_length = len(all_rows[0])
    num_samples = total_signal_length // 3

    columns = ['file', 'case_index']
    ecg_columns    = [f'ecg_{j+1}' for j in range(num_samples)]
    ppg_columns    = [f'ppg_{j+1}' for j in range(num_samples)]
    bioimp_columns = [f'bioimp_{j+1}' for j in range(num_samples)]
    columns.extend(ecg_columns)
    columns.extend(ppg_columns)
    columns.extend(bioimp_columns)

    data_rows = []
    for (file_path, case_idx), signal_row in zip(row_info, all_rows):
        row = [file_path, case_idx] + list(signal_row)
        data_rows.append(row)

    df = pd.DataFrame(data_rows, columns=columns)

    if output_csv is not None:
        try:
            print(f"DataFrame successfully saved to {output_csv}")
        except Exception as e:
            print(f"Error saving CSV to {output_csv}: {e}")

    return df
import os
import pandas as pd

def data_preparation_sbp_dbp(folder_path, columns):
    result = folder_path.split('/')[-1]
    name = '_'.join(result.split('_')[:-1])
    patient1_data = os.path.join(folder_path, f'{name}P1.txt')
    patient2_data = os.path.join(folder_path, f'{name}P2.txt')

    df1 = pd.read_csv(patient1_data, delimiter=';', skiprows=8)
    df1=df1.iloc[:,0:11]
    df1.columns = columns
    df2 = pd.read_csv(patient2_data, delimiter=';', skiprows=8)
    df2=df2.iloc[:,0:11]
    df2.columns = columns

    # Function to find available .mat files for a given patient
    def get_available_mat_files(folder, prefix):
        return sorted([os.path.join(folder, f) for f in os.listdir(folder) if f.startswith(prefix) and f.endswith('.mat')])

    # Get available mat files for each patient
    p1_matfiles = get_available_mat_files(folder_path, 'test1_')
    p2_matfiles = get_available_mat_files(folder_path, 'test2_')

    # Process available .mat files
    df_combinedp1 = process_mat_files(p1_matfiles, fs=1000, cutoff=5, order=3, output_csv='combined_signals.csv') if p1_matfiles else pd.DataFrame()
    df_combinedp2 = process_mat_files(p2_matfiles, fs=1000, cutoff=5, order=3, output_csv='combined_signals.csv') if p2_matfiles else pd.DataFrame()

    # Filter out rows where 'Systolic (mmHg)' is 0
    df1 = df1[df1['Systolic (mmHg)'] != 0].reset_index(drop=True)
    df2 = df2[df2['Systolic (mmHg)'] != 0].reset_index(drop=True)

    # Trim patient data to match signal data length
    if not df_combinedp1.empty:
        df1_trimmed = df1.iloc[:len(df_combinedp1)]
        final_dfp1 = pd.concat([df_combinedp1, df1_trimmed], axis=1)
    else:
        final_dfp1 = pd.DataFrame()  # Empty DataFrame if no .mat files found

    if not df_combinedp2.empty:
        df2_trimmed = df2.iloc[:len(df_combinedp2)]
        final_dfp2 = pd.concat([df_combinedp2, df2_trimmed], axis=1)
    else:
        final_dfp2 = pd.DataFrame()  # Empty DataFrame if no .mat files found

    # Combine both patients' data
    combined_final_df_p1_p2 = pd.concat([final_dfp1, final_dfp2]) if not final_dfp1.empty or not final_dfp2.empty else pd.DataFrame()

    return combined_final_df_p1_p2
import os
main_folder = "/content/BP_data_Cabrini2017_New"

all_folder_paths = []

for root, dirs, _ in os.walk(main_folder):
    for directory in dirs:
        full_path = os.path.join(root, directory)
        all_folder_paths.append(full_path)
print(all_folder_paths)
len(all_folder_paths)
columns=['Time (s)', 'Systolic (mmHg)', 'Diastolic (mmHg)', 'Mean (mmHg)',
       'Heart rate (bpm)', 'Stroke Volume (ml)', 'LVET (ms)',
       'Pulse Interval (ms)', 'Maximum Slope (mmHg/s)',
       'Cardiac Output (l/min)', 'TPR (dyn.s/cm5)']
len(columns)
from tqdm import tqdm  # Import tqdm for progress bar

df_list = []

# Process each folder path with a progress bar
for folder_path in tqdm(all_folder_paths, desc="Processing Folders", unit="folder"):
    df = data_preparation_sbp_dbp(folder_path, columns)
    df_list.append(df)

# Concatenate all data into a final DataFrame
df_data = pd.concat(df_list, ignore_index=True)
df_data
df_data=df_data.drop(['file','case_index','Time (s)'],axis=1)
df_data
df_data.shape
df=df_data.copy()
import numpy as np
import pandas as pd
from scipy.signal import find_peaks
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error

df=df_data.copy()

# Define feature categories
target_columns = ["Systolic (mmHg)", "Diastolic (mmHg)"]  # SBP and DBP as targets
medical_features = [
    "Mean (mmHg)", "Heart rate (bpm)", "Stroke Volume (ml)", "LVET (ms)",
    "Pulse Interval (ms)", "Maximum Slope (mmHg/s)", "Cardiac Output (l/min)", "TPR (dyn.s/cm5)"
]
ecg_features = [f"ecg_{i}" for i in range(1, 1001)]
ppg_features = [f"ppg_{i}" for i in range(1, 1001)]
bioimpedance_features = [f"bioimp_{i}" for i in range(1, 1001)]

# Function to extract key features from signals
def extract_signal_features(signal_values):
    peaks, _ = find_peaks(signal_values)
    num_peaks = len(peaks)
    peak_mean = np.mean(signal_values[peaks]) if num_peaks > 0 else 0
    peak_std = np.std(signal_values[peaks]) if num_peaks > 0 else 0
    min_val = np.min(signal_values)
    max_val = np.max(signal_values)
    return num_peaks, peak_mean, peak_std, min_val, max_val

# Extract features for ECG, PPG, and Bioimpedance
extracted_features = []

for index, row in df.iterrows():
    ecg_values = row[ecg_features].values
    ppg_values = row[ppg_features].values
    bioimpedance_values = row[bioimpedance_features].values

    ecg_feats = extract_signal_features(ecg_values)
    ppg_feats = extract_signal_features(ppg_values)
    bio_feats = extract_signal_features(bioimpedance_values)

    extracted_features.append(list(ecg_feats) + list(ppg_feats) + list(bio_feats))

# Create new DataFrame with extracted features
feature_columns = [
    "ecg_peaks", "ecg_peak_mean", "ecg_peak_std", "ecg_min", "ecg_max",
    "ppg_peaks", "ppg_peak_mean", "ppg_peak_std", "ppg_min", "ppg_max",
    "bio_peaks", "bio_peak_mean", "bio_peak_std", "bio_min", "bio_max"
]

df_extracted = pd.DataFrame(extracted_features, columns=feature_columns)

# Combine with medical features and target columns
final_df1 = pd.concat([df[medical_features], df_extracted, df[target_columns]], axis=1)

final_df1
final_df1=final_df1.dropna(axis=0)
final_df1.isnull().sum().sum()
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Normalize features
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X = scaler_X.fit_transform(final_df1.drop(columns=target_columns))
y = scaler_y.fit_transform(final_df1[target_columns])

# Define test set size
test_size = 3583

# Split dataset: First, separate the test set
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

# Determine the size of training and validation sets
val_size = 0.125  # Since we need 10% validation out of the remaining data

# Split remaining data into training (70%) and validation (10%)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size, random_state=42)

# Print dataset sizes to verify
print(f"Training set size: {X_train.shape[0]}")
print(f"Validation set size: {X_val.shape[0]}")
print(f"Testing set size: {X_test.shape[0]}")
# Reshape data for CNN (1D convolution expects (samples, timesteps, features))
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

# Define 1D CNN Model
model = keras.Sequential([
    layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),
    layers.MaxPooling1D(pool_size=2),
    layers.Conv1D(filters=32, kernel_size=3, activation='relu'),
    layers.MaxPooling1D(pool_size=2),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(2, activation='linear')  # Predicting SBP and DBP
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))

# Evaluate the model
y_pred_scaled = model.predict(X_test)
y_pred = scaler_y.inverse_transform(y_pred_scaled)
y_test_unscaled = scaler_y.inverse_transform(y_test)

def calculate_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    sde = np.std(y_true - y_pred)
    return mse, rmse, mae, sde

# Compute metrics for SBP and DBP
metrics_sbp = calculate_metrics(y_test_unscaled[:, 0], y_pred[:, 0])
metrics_dbp = calculate_metrics(y_test_unscaled[:, 1], y_pred[:, 1])

# Print metrics
print(f"SBP - MSE: {metrics_sbp[0]:.4f}, RMSE: {metrics_sbp[1]:.4f}, MAE: {metrics_sbp[2]:.4f}, SDE: {metrics_sbp[3]:.4f}")
print(f"DBP - MSE: {metrics_dbp[0]:.4f}, RMSE: {metrics_dbp[1]:.4f}, MAE: {metrics_dbp[2]:.4f}, SDE: {metrics_dbp[3]:.4f}")

import numpy as np
import matplotlib.pyplot as plt

# Define the SBP valid range
sbp_min, sbp_max = 80, 150

# Filter based on original values
valid_indices_original = (y_test_unscaled[:, 0] >= sbp_min) & (y_test_unscaled[:, 0] <= sbp_max)
y_test_filtered_original = y_test_unscaled[valid_indices_original]
y_pred_filtered_original = y_pred[valid_indices_original]

# Filter based on predicted values
valid_indices_pred = (y_pred_filtered_original[:, 0] >= sbp_min) & (y_pred_filtered_original[:, 0] <= sbp_max)
y_test_filtered_final = y_test_filtered_original[valid_indices_pred]
y_pred_filtered_final = y_pred_filtered_original[valid_indices_pred]

# Plot the filtered data
plt.figure(figsize=(8, 6))
plt.scatter(y_test_filtered_final[:, 0], y_pred_filtered_final[:, 0], alpha=0.5, label="SBP Data", color='blue')
plt.plot([min(y_test_filtered_final[:, 0]), max(y_test_filtered_final[:, 0])+15],
         [min(y_test_filtered_final[:, 0]), max(y_test_filtered_final[:, 0])+15], 'r--', label="Ideal Prediction")
plt.xlabel("Original SBP")
plt.ylabel("Predicted SBP")
plt.title("Original vs Predicted SBP")
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

dbp_min, dbp_max = 50, 100

# Filter based on original values
valid_indices_original_dbp = (y_test_unscaled[:, 1] >= dbp_min) & (y_test_unscaled[:, 1] <= dbp_max)
y_test_filtered_original_dbp = y_test_unscaled[valid_indices_original_dbp]
y_pred_filtered_original_dbp = y_pred[valid_indices_original_dbp]

# Filter based on predicted values
valid_indices_pred_dbp = (y_pred_filtered_original_dbp[:, 1] >= dbp_min) & (y_pred_filtered_original_dbp[:, 1] <= dbp_max)
y_test_filtered_final_dbp = y_test_filtered_original_dbp[valid_indices_pred_dbp]
y_pred_filtered_final_dbp = y_pred_filtered_original_dbp[valid_indices_pred_dbp]

# Plot the filtered data
plt.figure(figsize=(8, 6))
plt.scatter(y_test_filtered_final_dbp[:, 1], y_pred_filtered_final_dbp[:, 1], alpha=0.5, label="DBP Data", color='green')
plt.plot([min(y_test_filtered_final_dbp[:, 1]), max(y_test_filtered_final_dbp[:, 1])+15],
         [min(y_test_filtered_final_dbp[:, 1]), max(y_test_filtered_final_dbp[:, 1])+15], 'r--', label="Ideal Prediction")
plt.xlabel("Original DBP")
plt.ylabel("Predicted DBP")
plt.title("Original vs Predicted DBP")
plt.legend(loc='lower right')
plt.grid(True)
plt.show()
